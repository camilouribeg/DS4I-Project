{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the notebook for the modeling, results and comparison of the different algorithms the idea is to follow the following parameters:\n",
    "1. Classification Models\n",
    "    1. Logistic Regression\n",
    "    2. RandomForrest \n",
    "    3. Tunning with GridSearchCV \n",
    "    4. Oversampling technique (if needed)\n",
    "2. Model Performance Comparison\n",
    "    1. F1 Score & Confusion Matrix / Recall\n",
    "    2. Camilo don’t remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the 5 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = pd.read_csv('../data/raw/scaled_features.csv', index_col=0)\n",
    "vif_features = pd.read_csv('../data/raw/vif_features.csv', index_col=0)\n",
    "polynomial_features = pd.read_csv('../data/raw/polynomial_features.csv', index_col=0)\n",
    "rfe_features = pd.read_csv('../data/raw/rfe_features.csv', index_col=0)\n",
    "pca_features = pd.read_csv('../data/raw/pca_features.csv', index_col=0)\n",
    "y = pd.read_csv('../data/raw/target_variable.csv').squeeze()  # Use .squeeze() to convert to a Series if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset (Train/Validation/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into training and validation sets directly using train_test_split\n",
    "def split_train_val_test(X, y, val_size=0.2, test_size=0.2, random_state=33):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets with stratification.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Target variable\n",
    "    - val_size: Proportion of the validation set (default 0.2)\n",
    "    - test_size: Proportion of the test set (default 0.2)\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    # First split to get the test set with stratification\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    # Second split to separate validation from training with stratification\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size, stratify=y_train_val, random_state=random_state)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scaled_features dataset\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train_scaled, y_val_scaled, y_test_scaled = split_train_val_test(scaled_features, y)\n",
    "\n",
    "# For vif_features dataset\n",
    "X_train_vif, X_val_vif, X_test_vif, y_train_vif, y_val_vif, y_test_vif = split_train_val_test(vif_features, y)\n",
    "\n",
    "# For polynomial_features dataset\n",
    "X_train_poly, X_val_poly, X_test_poly, y_train_poly, y_val_poly, y_test_poly = split_train_val_test(polynomial_features, y)\n",
    "\n",
    "# For rfe_features dataset\n",
    "X_train_rfe, X_val_rfe, X_test_rfe, y_train_rfe, y_val_rfe, y_test_rfe = split_train_val_test(rfe_features, y)\n",
    "\n",
    "# For pca_features dataset\n",
    "X_train_pca, X_val_pca, X_test_pca, y_train_pca, y_val_pca, y_test_pca = split_train_val_test(pca_features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import LogisticRegression from sklearn and initialize the model for all datasets. After that, you can train the model for each dataset using your split data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 4: Initialize Logistic Regression for each dataset\n",
    "def initialize_logistic_regression():\n",
    "    \"\"\"\n",
    "    Initializes Logistic Regression with class weight balanced to handle class imbalance.\n",
    "    \n",
    "    Returns:\n",
    "    - LogisticRegression object\n",
    "    \"\"\"\n",
    "    # Logistic Regression model with class_weight='balanced' to handle class imbalance\n",
    "    model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "    return model\n",
    "\n",
    "# Initialize the Logistic Regression model for all datasets\n",
    "logistic_model_scaled = initialize_logistic_regression()\n",
    "logistic_model_vif = initialize_logistic_regression()\n",
    "logistic_model_poly = initialize_logistic_regression()\n",
    "logistic_model_rfe = initialize_logistic_regression()\n",
    "logistic_model_pca = initialize_logistic_regression()\n",
    "\n",
    "# Now you have Logistic Regression initialized for each dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting bankruptcy it is necessary to minimize the False Negatives (predicting a company is not going to bankruptcy when it actually will) so the key metric for model performance and selection will be RECALL.   On the other hand we will use f1 score to balance performance providing a measure that takes into account imbalanced data (which is our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning and Cross Validation GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid for Logistic Regression with different class weights\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],   # Regularization type\n",
    "    'class_weight': [\n",
    "        'balanced',  # Automatically adjust class weights inversely proportional to class frequencies\n",
    "       \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled_Features (no data engineering or feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                          max_iter=1000, solver=&#x27;liblinear&#x27;),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1], &#x27;class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             refit=&#x27;f1&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;f1&#x27;: make_scorer(f1_score),\n",
       "                      &#x27;recall&#x27;: make_scorer(recall_score)},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                          max_iter=1000, solver=&#x27;liblinear&#x27;),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1], &#x27;class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             refit=&#x27;f1&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;f1&#x27;: make_scorer(f1_score),\n",
       "                      &#x27;recall&#x27;: make_scorer(recall_score)},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight='balanced',\n",
       "                                          max_iter=1000, solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1], 'class_weight': ['balanced'],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             refit='f1', return_train_score=True,\n",
       "             scoring={'f1': make_scorer(f1_score),\n",
       "                      'recall': make_scorer(recall_score)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define scoring metrics for recall and F1\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=logistic_model_scaled,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=scoring,  # Use both recall and F1\n",
    "    refit='f1',  # Refit the model using the best F1 score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True  # Include training scores in results\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_lr.fit(X_train_scaled, y_train_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for scaled_features: {'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the best model and evaluate\n",
    "best_model_scaled = grid_search_lr.best_estimator_\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params_scaled = grid_search_lr.best_params_\n",
    "print(f\"Best hyperparameters for scaled_features: {best_params_scaled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Recall for scaled_features: 0.8936\n",
      "Training F1 Score for scaled_features: 0.2979\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate the best model on the training set ---\n",
    "y_train_pred_scaled = best_model_scaled.predict(X_train_scaled)\n",
    "train_recall_scaled = recall_score(y_train_scaled, y_train_pred_scaled)\n",
    "train_f1_scaled = f1_score(y_train_scaled, y_train_pred_scaled)\n",
    "\n",
    "print(f\"Training Recall for scaled_features: {train_recall_scaled:.4f}\")\n",
    "print(f\"Training F1 Score for scaled_features: {train_f1_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for scaled_features: 0.8571\n",
      "F1 Score for scaled_features: 0.3046\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "y_val_pred_scaled = best_model_scaled.predict(X_val_scaled)\n",
    "recall_scaled = recall_score(y_val_scaled, y_val_pred_scaled)\n",
    "f1_scaled = f1_score(y_val_scaled, y_val_pred_scaled)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Recall for scaled_features: {recall_scaled:.4f}\")\n",
    "print(f\"F1 Score for scaled_features: {f1_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'penalt...</td>\n",
       "      <td>0.897187</td>\n",
       "      <td>0.299733</td>\n",
       "      <td>0.851724</td>\n",
       "      <td>0.289672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'penalty'...</td>\n",
       "      <td>0.890107</td>\n",
       "      <td>0.308217</td>\n",
       "      <td>0.809113</td>\n",
       "      <td>0.279349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'penalt...</td>\n",
       "      <td>0.900727</td>\n",
       "      <td>0.297472</td>\n",
       "      <td>0.851724</td>\n",
       "      <td>0.277584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'penalty'...</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>0.310018</td>\n",
       "      <td>0.809113</td>\n",
       "      <td>0.276087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  train_recall  train_f1  \\\n",
       "0  {'C': 0.1, 'class_weight': 'balanced', 'penalt...      0.897187  0.299733   \n",
       "2  {'C': 1, 'class_weight': 'balanced', 'penalty'...      0.890107  0.308217   \n",
       "1  {'C': 0.1, 'class_weight': 'balanced', 'penalt...      0.900727  0.297472   \n",
       "3  {'C': 1, 'class_weight': 'balanced', 'penalty'...      0.895433  0.310018   \n",
       "\n",
       "   val_recall    val_f1  \n",
       "0    0.851724  0.289672  \n",
       "2    0.809113  0.279349  \n",
       "1    0.851724  0.277584  \n",
       "3    0.809113  0.276087  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detailed_results(grid_search):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with recall and F1 scores for training and validation sets from a fitted GridSearchCV object.\n",
    "    \n",
    "    Parameters:\n",
    "    - grid_search: Fitted GridSearchCV object.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing hyperparameters, recall and F1 scores for training and validation sets.\n",
    "    \"\"\"\n",
    "    # Convert cv_results_ to a DataFrame\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    # Extract and rename the relevant columns\n",
    "    results_df = results_df[['params', \n",
    "                             'mean_train_recall', 'mean_train_f1', \n",
    "                             'mean_test_recall', 'mean_test_f1']]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    results_df.columns = ['params', 'train_recall', 'train_f1', 'val_recall', 'val_f1']\n",
    "    \n",
    "    return results_df.sort_values(by='val_f1', ascending=False)\n",
    "\n",
    "# Now you can run the function as follows:\n",
    "results_df_scaled = get_detailed_results(grid_search_lr)\n",
    "\n",
    "# Display the top results\n",
    "results_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_scaled.to_csv('../data/results/results_df_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate the impact of generating feature engineering and creating interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features Dataset (Including feature engineering interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate the model in a reduced gridsearch to not involve all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid_lr_poly = {\n",
    "    'C': [0.1, 1],  # Regularization strength\n",
    "    'penalty': ['l1'],   # Regularization type\n",
    "    'class_weight': [\n",
    "        'balanced',  # Automatically adjust class weights inversely proportional to class frequencies\n",
    "       \n",
    "    ]\n",
    "}\n",
    "grid_search_lr_poly = GridSearchCV(\n",
    "    estimator=logistic_model_poly,\n",
    "    param_grid=param_grid_lr_poly,\n",
    "    scoring=scoring,  # Use both recall and F1\n",
    "    refit='f1',  # Refit the model using the best F1 score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_lr_poly.fit(X_train_poly, y_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_poly = get_detailed_results(grid_search_lr_poly)\n",
    "results_df_poly.to_csv('../data/results/results_df_poly.csv')\n",
    "results_df_poly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF Features (Selected using VIF technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate on vif_features dataset\n",
    "grid_search_lr_vif = GridSearchCV(\n",
    "    estimator=logistic_model_vif,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring= scoring,  # Use both recall and F1\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model on scaled_features\n",
    "grid_search_lr_vif.fit(X_train_vif, y_train_vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_vif = get_detailed_results(grid_search_lr_vif)\n",
    "results_df_vif.to_csv('../data/results/results_df_vif.csv')\n",
    "results_df_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Features (Selected using PCA technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate on vif_features dataset\n",
    "grid_search_lr_pca = GridSearchCV(\n",
    "    estimator=logistic_model_pca,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring= scoring,  # Use both recall and F1\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model on scaled_features\n",
    "grid_search_lr_pca.fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_pca = get_detailed_results(grid_search_lr_pca)\n",
    "results_df_pca.to_csv('../data/results/results_df_pca.csv')\n",
    "results_df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE Features (Selected features using RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate on vif_features dataset\n",
    "grid_search_lr_rfe = GridSearchCV(\n",
    "    estimator=logistic_model_rfe,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring= scoring,  # Use both recall and F1\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model on scaled_features\n",
    "grid_search_lr_rfe.fit(X_train_rfe, y_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_rfe = get_detailed_results(grid_search_lr_rfe)\n",
    "results_df_rfe.to_csv('../data/results/results_df_rfe.csv')\n",
    "results_df_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous section we are only going to apply this model to the best results found on Logistic Regression and compare final results and which model can predict better the bankruptcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Technique "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Import preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Display all columns and rows in the dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset from correlation_with_bankrupt.csv\n",
    "file_path = '../data/raw/cleaned_data.csv'\n",
    "\n",
    "#Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Display the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the shape of the dataset\n",
    "print(\"Shape of the dataset: \", df.shape)\n",
    "\n",
    "#Check the columns of the dataset\n",
    "print(\"\\nColumns in the dataset:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe that there is an additional 'space' at the beginning of the column names. Need to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the datatypes of the columns\n",
    "print(f\"Datatypes of the columns:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the columns have dtype \"float64\" except for: \" Liability-Assets Flag\" and \"Bankrupt?\" which is target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the unique values in the column \" Liability-Assets Flag\"\n",
    "print(f\"Unique values in the column ' Liability-Assets Flag':\\n{df[' Liability-Assets Flag'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in the dataset\n",
    "print(f\"Missing values in the dataset:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate rows in the dataset\n",
    "print(f\"Duplicate rows in the dataset: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics of the dataset\n",
    "summary_statistics = df.describe()\n",
    "summary_statistics.to_csv('../data/raw/summary_statistics.csv')\n",
    "\n",
    "#Display the summary statistics from csv file\n",
    "summary_statistics = pd.read_csv('../data/raw/summary_statistics.csv')\n",
    "print(summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of the target variable\n",
    "\n",
    "#Count the number of bankrupt and non-bankrupt companies\n",
    "bankrupt = df['Bankrupt?'].value_counts()\n",
    "print(f\"Number of bankrupt and non-bankrupt companies:\\n{bankrupt}\")\n",
    "\n",
    "#Count the normalized value of bankrupt and non-bankrupt companies\n",
    "bankrupt_normalized = df['Bankrupt?'].value_counts(normalize=True)\n",
    "print(f\"\\nNormalized value of bankrupt and non-bankrupt companies:\\n{bankrupt_normalized}\")\n",
    "\n",
    "#Plot the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Bankrupt?', data=df)\n",
    "plt.title('Distribution of the target variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only 3.2% of the values account to '1' that is 'Bankrupt companies' and rest 96.8% account to '0' that is 'Non-bankrupt companies'.\n",
    "- We need to keep this imbalance of the target variable in mind during modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers in the dataset in a non-graphical way\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "#Display the number of outliers in each column\n",
    "print(f\"Number of outliers in each column:\\n{outliers}\")\n",
    "\n",
    "#Display the percentage of outliers in each column\n",
    "percentage_outliers = (outliers/df.shape[0])*100\n",
    "print(f\"\\nPercentage of outliers in each column:\\n{percentage_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the distribution of the features using histograms\n",
    "df.hist(figsize=(20, 20))\n",
    "plt.suptitle('Distribution of the features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for features with more than 5% outliers\n",
    "features_above_5pct_outliers = percentage_outliers[percentage_outliers > 5]\n",
    "print(f\"Features with more than 5% outliers:\\n{features_above_5pct_outliers}\")\n",
    "\n",
    "#Print the count of features with more than 5% outliers\n",
    "print(f\"\\nNumber of features with more than 5% outliers: {len(features_above_5pct_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the outliers from the dataset\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_no_outliers = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the shape of the dataset after removing the outliers\n",
    "print(f\"Shape of the dataset after removing outliers: {df_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of the target variable after removing the outliers\n",
    "print(f\"Distribution of the target variable after removing the outliers:\\n{df_no_outliers['Bankrupt?'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After removing outliers all the records with target variable '1' are removed. This means the records are not outliers but real life 'Bankruptcy' cases.\n",
    "- Hence we need to consider all the records in the dataset for our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into feature and target variables\n",
    "X = df.drop('Bankrupt?', axis=1)\n",
    "y = df['Bankrupt?']\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the shape of X_scaled and y\n",
    "print(f\"Shape of X_scaled: {X_scaled.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 5 rows of the scaled feature variables\n",
    "print(f\"First 5 rows of the scaled features: \\n{X_scaled.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the scaled features to a csv file\n",
    "X_scaled.to_csv('../data/raw/scaled_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False).set_output(transform='pandas')\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "#Display the shape of the dataset after creating polynomial features\n",
    "print(f\"Shape of the dataset after creating polynomial features: {X_poly.shape}\")\n",
    "\n",
    "#Display the first 5 rows of the dataset after creating polynomial features\n",
    "print(f\"First 5 rows of the dataset after creating polynomial features: \\n{X_poly.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the polynomial features to a csv file\n",
    "X_poly.to_csv('../data/raw/polynomial_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import OLS from statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Add a constant to the dataset\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "\n",
    "#Checking the indices of X_poly are aligned with y_no_outliers\n",
    "print(f\"Are the indices of X_poly aligned with target: {X_poly.index.equals(y.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A. VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a Variance Inflation check on X_poly and X_scaled\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#Create a copy of X_poly\n",
    "X_poly_copy = X_poly.copy()\n",
    "\n",
    "#Add a constant to the dataset\n",
    "X_poly_copy = sm.add_constant(X_poly_copy)\n",
    "\n",
    "#Eliminating features based on VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X_poly_copy.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_poly_copy.values, i) for i in range(X_poly_copy.shape[1])]\n",
    "vif = vif.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "#Display the features and their VIF values\n",
    "print(f\"Features and their VIF values:\\n{vif}\")\n",
    "\n",
    "#Drop the features with VIF greater than 10\n",
    "X_poly_copy = X_poly_copy.drop(columns=vif[vif['VIF'] > 10]['Features'])\n",
    "\n",
    "#Display the shape of the dataset after dropping features with VIF greater than 10\n",
    "print(f\"Shape of the dataset after dropping features with VIF greater than 10: {X_poly_copy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the features of X_poly_copy dataset after dropping features with VIF greater than 10\n",
    "print(f\"Features of X_poly_copy dataset after dropping features with VIF greater than 10:\\n{X_poly_copy.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset after dropping features with VIF greater than 10 to a csv file\n",
    "X_poly_copy.to_csv('../data/raw/vif_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Create a PCA object\n",
    "pca = PCA(n_components=0.95).set_output(transform=\"pandas\")\n",
    "\n",
    "#Fit the PCA object to the scaled features\n",
    "X_pca = pca.fit_transform(X_poly)\n",
    "\n",
    "#Display the shape of the dataset after PCA\n",
    "print(f\"Shape of the dataset after PCA: {X_pca.shape}\")\n",
    "\n",
    "#Display the first 5 rows of the dataset after PCA\n",
    "print(f\"First 5 rows of the dataset after PCA: \\n{X_pca.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset after PCA to a csv file\n",
    "X_pca.to_csv('../data/raw/pca_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2C. RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE for feature selection\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#Create a logistic regression object\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Create a RFE object\n",
    "rfecv = RFECV(estimator=logreg, \n",
    "              step=1, \n",
    "              cv=StratifiedKFold(5), \n",
    "              scoring='recall'\n",
    "              ).set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE on X_scaled\n",
    "X_rfecv_scaled = rfecv.fit_transform(X_scaled, y)\n",
    "\n",
    "#Get the optimal number of features\n",
    "optimal_num_features_scaled = rfecv.n_features_\n",
    "print(f\"Optimal number of features: {optimal_num_features_scaled}\")\n",
    "\n",
    "#Get the selected features\n",
    "selected_features_scaled = rfecv.support_\n",
    "print(f\"Selected features: {selected_features_scaled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the shape of the dataset after RFE\n",
    "print(f\"Shape of the dataset after RFE: {X_rfecv_scaled.shape}\")\n",
    "\n",
    "#Display the selected features after RFE\n",
    "print(f\"Selected features after RFE: \\n{X_rfecv_scaled.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset after RFE to a csv file\n",
    "X_rfecv_scaled.to_csv('../data/raw/rfe_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save target variable to a csv file\n",
    "y.to_csv('../data/raw/target_variable.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection for scaled dataset without Polynomial features (interaction effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a Variance Inflation check on X_poly and X_scaled\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#Create a copy of X_poly\n",
    "X_scaled_copy = X_scaled.copy()\n",
    "\n",
    "#Add a constant to the dataset\n",
    "X_scaled_copy = sm.add_constant(X_scaled_copy)\n",
    "\n",
    "#Eliminating features based on VIF\n",
    "vif_new = pd.DataFrame()\n",
    "vif_new[\"Features\"] = X_scaled_copy.columns\n",
    "vif_new[\"VIF\"] = [variance_inflation_factor(X_scaled_copy.values, i) for i in range(X_scaled_copy.shape[1])]\n",
    "vif_new = vif_new.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "#Display the features and their VIF values\n",
    "print(f\"Features and their VIF values:\\n{vif_new}\")\n",
    "\n",
    "#Drop the features with VIF greater than 10\n",
    "X_scaled_copy = X_scaled_copy.drop(columns=vif_new[vif_new['VIF'] > 10]['Features'])\n",
    "\n",
    "#Display the shape of the dataset after dropping features with VIF greater than 10\n",
    "print(f\"Shape of the dataset after dropping features with VIF greater than 10: {X_scaled_copy.shape}\")\n",
    "\n",
    "#Display the features of X_poly_copy dataset after dropping features with VIF greater than 10\n",
    "print(f\"Features of X_poly_copy dataset after dropping features with VIF greater than 10:\\n{X_scaled_copy.columns}\")\n",
    "\n",
    "#Save the dataset after dropping features with VIF greater than 10 to a csv file\n",
    "X_scaled_copy.to_csv('../data/features_without_interactions/vif_features_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Create a PCA object\n",
    "pca_new = PCA(n_components=0.95).set_output(transform=\"pandas\")\n",
    "\n",
    "#Fit the PCA object to the scaled features\n",
    "X_pca_new = pca_new.fit_transform(X_scaled)\n",
    "\n",
    "#Display the shape of the dataset after PCA\n",
    "print(f\"Shape of the dataset after PCA: {X_pca_new.shape}\")\n",
    "\n",
    "#Display the first 5 rows of the dataset after PCA\n",
    "print(f\"First 5 rows of the dataset after PCA: \\n{X_pca_new.head()}\")\n",
    "\n",
    "#Save the dataset after PCA to a csv file\n",
    "X_pca_new.to_csv('../data/features_without_interactions/pca_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3C. RFECV for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE for feature selection\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a logistic regression object\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "#Create a RFECV object\n",
    "rfecv_new = RFECV(estimator=rfc, \n",
    "              step=1, \n",
    "              cv=StratifiedKFold(5), \n",
    "              scoring='recall'\n",
    "              ).set_output(transform=\"pandas\")\n",
    "\n",
    "#RFECV on X_scaled\n",
    "X_rfecv_scaled_new = rfecv_new.fit_transform(X_scaled, y)\n",
    "\n",
    "#Get the optimal number of features\n",
    "optimal_num_features_scaled_new = rfecv_new.n_features_\n",
    "print(f\"Optimal number of features: {optimal_num_features_scaled_new}\")\n",
    "\n",
    "#Get the selected features\n",
    "selected_features_scaled_new = rfecv_new.support_\n",
    "print(f\"Selected features: {selected_features_scaled_new}\")\n",
    "\n",
    "#Display the shape of the dataset after RFECV\n",
    "print(f\"Shape of the dataset after RFECV: {X_rfecv_scaled_new.shape}\")\n",
    "\n",
    "#Display the selected features after RFECV\n",
    "print(f\"Selected features after RFECV: \\n{X_rfecv_scaled_new.columns}\")\n",
    "\n",
    "#Save the dataset after RFE to a csv file\n",
    "X_rfecv_scaled.to_csv('../data/features_without_interactions/rfe_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
